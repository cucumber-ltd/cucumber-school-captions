WEBVTT

1
00:00:14.592 --> 00:00:17.920
Welcome to the final lesson in
this series of Cucumber School

2
00:00:18.176 --> 00:00:24.320
Over the series we've tried to cover
all the important techniques

3
00:00:24.576 --> 00:00:27.136
and concepts we think you need to become a
successful behaviour-driven developer

4
00:00:28.160 --> 00:00:31.488
We've taught you how to break down
requirements with example mapping

5
00:00:32.000 --> 00:00:35.584
and how to express those examples
as Gherkin scenarios

6
00:00:36.096 --> 00:00:39.680
We've explained the importance of
keeping your features readable

7
00:00:39.936 --> 00:00:44.544
and shown you how to write great, flexible,
step definitions to help you achieve that goal

8
00:00:45.568 --> 00:00:51.712
We've also explored the difference between
acceptance tests and unit tests

9
00:00:51.968 --> 00:00:58.112
and demonstrated how the outside in
approach to software development works

using both types of tests
to drive out the solution to your

10
00:00:58.368 --> 00:00:59.648
stakeholders' problems

11
00:01:01.696 --> 00:01:04.512
One glaring omission from
the story so far however

12
00:01:04.768 --> 00:01:08.352
is that our Shouty solution is
nothing more than a domain model

13
00:01:09.120 --> 00:01:12.192
It doesn't have any way for
a user to interact with it

14
00:01:12.960 --> 00:01:15.776
Well, that's all about to change

16
00:01:16.544 --> 00:01:22.688
In this episode, we'll review the code
that's just been written

for the first iteration of
Shouty's web user interface

17
00:01:22.944 --> 00:01:26.784
and show you how to use Selenium WebDriver

18
00:01:27.296 --> 00:01:33.440
a browser automation library to run our
Cucumber scenarios through that user interface

19
00:01:34.464 --> 00:01:40.608
We think it would be irresponsible
to teach you how to use Selenium

20
00:01:40.864 --> 00:01:43.680
without also teaching you about the Agile
testing pyramid and its nemesis

21
00:01:43.936 --> 00:01:45.984
the testing ice cream cone

22
00:01:46.752 --> 00:01:52.896
We've come across too many teams
who have ended up with

23
00:01:53.152 --> 00:01:55.456
miserably slow, unreliable test suites
that cost too much to maintain

24
00:01:55.712 --> 00:01:59.296
because all their Cucumber scenarios
go through the UI

25
00:02:00.064 --> 00:02:01.856
It doesn't have to be this way
and in this episode

26
00:02:02.112 --> 00:02:05.696
we'll show you how to avoid this trap

27
00:02:06.720 --> 00:02:12.864
Let's start by walking you through
the changes that have been happening

in the code base while we've been away

28
00:02:14.400 --> 00:02:19.264
Shouty now has a simple web UI
which displays a form where

a user can shout a message

29
00:02:19.520 --> 00:02:24.128
It won't win any awards for style just yet
but it should be functional at least

30
00:02:25.664 --> 00:02:31.808
We can let Cucumber put the new web app
through its paces

31
00:02:32.064 --> 00:02:33.088
by setting the shouty.testDepth
system property to web

32
00:02:33.344 --> 00:02:34.880
when we run Cucumber

33
00:02:36.160 --> 00:02:40.768
This setting causes the scenarios
to be run through the browser via Selenium

34
00:02:41.280 --> 00:02:45.888
and if you watch closely

you can see the message being typed
into the form as it runs

35
00:02:46.912 --> 00:02:53.056
You can see it's way slower
to run the scenarios via browser

36
00:03:06.112 --> 00:03:12.000
Luckily, we still have our original,
much faster version of the acceptance tests

that go directly to the domain model

37
00:03:12.768 --> 00:03:14.560
These run in less than a second

38
00:03:14.816 --> 00:03:20.960
We can run this version by setting
'shouty_test_depth' to something else

or just leaving it out altogether

39
00:03:25.568 --> 00:03:26.592
Nice!

40
00:03:26.848 --> 00:03:29.664
So we have the best of both worlds it seems

41
00:03:30.432 --> 00:03:36.576
Let's have a look at how this
has been implemented in the features

42
00:03:36.832 --> 00:03:39.392
In the constructor of the Stepdefs class, here,
we’re declaring a parameter of type ShoutSupport

43
00:03:39.648 --> 00:03:44.256
This is an abstract class, and we have two
concrete implementations of that class -

44
00:03:44.512 --> 00:03:47.840
DomainShoutSupport and WebShoutSupport

45
00:03:48.864 --> 00:03:55.008
Over here, in CustomPicoFactory, we decide
which one to use based on the

46
00:03:55.264 --> 00:03:56.032
shouty.testDepth system property

47
00:03:57.056 --> 00:04:02.176
We’re telling Cucumber to use this custom
implementation of ObjectFactory

here in cucumber.properties

48
00:04:03.712 --> 00:04:07.808
Both DomainShoutSupport and WebShoutSupport
are polymorphic -

49
00:04:08.320 --> 00:04:12.416
meaning they implement the exact same set of
methods defined in ShoutSupport

50
00:04:13.184 --> 00:04:16.768
As far as the step definitions are concerned
they’re interchangeable

51
00:04:17.024 --> 00:04:22.143
This allows us to use different strategies for automating the application

52
00:04:24.191 --> 00:04:29.311
DomainShoutSupport contains the familiar
automation code that calls

the domain model directly

53
00:04:31.359 --> 00:04:37.503
WebShoutSupport, on the other hand, is where we
automate Shouty through its new web UI

55
00:04:38.015 --> 00:04:42.111
As you can see, there’s quite a bit more work
being done here

56
00:04:43.647 --> 00:04:49.791
These hooks, startServer and stopServer
start and stop the web server

57
00:04:50.047 --> 00:04:53.119
so that we can be confident there will be no
state leaking from one scenario to the next

58
00:04:54.143 --> 00:05:00.287
This hook, closeAllBrowsers makes sure to close
any open browsers at the end of each scenario

59
00:05:01.567 --> 00:05:06.943
The implementation of seanShout is very different
to the one that hits the domain model directly

60
00:05:07.711 --> 00:05:10.783
First, we “log in” as Sean -

61
00:05:11.039 --> 00:05:15.391
in fact this just opens the homepage for the app,
putting the user’s name in the query string

62
00:05:16.927 --> 00:05:21.791
The browser here is an instance of Selenium’s
FirefoxDriver class

63
00:05:22.559 --> 00:05:27.679
We keep all the browsers used in the scenario
in a Map, with one browser per person

65
00:05:28.703 --> 00:05:32.799
We use the getBrowserFor method to fetch
a browser for Sean

66
00:05:33.055 --> 00:05:35.615
then assign that as the current browser

67
00:05:37.151 --> 00:05:43.295
Next, the shout method asks WebDriver to
find the message box on the form

68
00:05:43.551 --> 00:05:45.855
then types out the contents of the message
into the box

69
00:05:46.367 --> 00:05:50.463
Next, it finds the form’s submit button
and clicks it

71
00:05:54.815 --> 00:05:58.655
Let’s walk through what happens when
a scenario runs using webShoutSupport

72
00:05:59.423 --> 00:06:05.567
When Cucumber runs the
“Sean shouts Free Bagels” step,

73
00:06:05.823 --> 00:06:07.615
it searches for the corresponding step definition
and executes the method in the StepDefs class

74
00:06:09.151 --> 00:06:14.271
Now, the code in that step definition calls
the seanShout method on shoutSupport

75
00:06:14.783 --> 00:06:17.599
which is an instance of WebShoutSupport,
which in turn creates a WebDriver

76
00:06:18.623 --> 00:06:24.767
and calls the get method on it to
open the homepage as Sean

77
00:06:25.279 --> 00:06:31.423
Selenium WebDriver now tells Firefox to
open a browser at that URL

78
00:06:31.679 --> 00:06:34.239
and when the browser opens that URL
our web server will get a request for the page

79
00:06:34.495 --> 00:06:37.823
We render the page on the server,
and the browser displays it

80
00:06:38.847 --> 00:06:43.455
Next, WebShoutSupport asks Selenium to fill out
and submit the shout form

81
00:06:43.967 --> 00:06:46.271
which causes button clicks in the browser

82
00:06:47.039 --> 00:06:52.415
The form is submitted to the server,
and the server calls the core domain

to broadcast the shout

83
00:06:53.695 --> 00:07:00.351
In contrast, domainShoutSupport’s implementation
of seanShout calls the domain model directly

85
00:07:01.119 --> 00:07:04.959
Notice this is exactly the same code as the web server uses

86
00:07:05.727 --> 00:07:11.871
The strategy that talks directly
to the core domain model

gives us fast feedback
and lets us test where the

87
00:07:12.127 --> 00:07:14.175
heart of the application's
behaviour is implemented

88
00:07:14.943 --> 00:07:21.087
We've now added a second strategy
that allows us to prove this behavior

still works when presented via the web UI

89
00:07:22.111 --> 00:07:24.159
You might be wondering why we need both?

90
00:07:24.671 --> 00:07:28.511
Why didn't we just start driving our way in
from the web UI in the beginning?

91
00:07:30.303 --> 00:07:35.679
We've found that focusing on driving out
a domain model from our scenarios first

92
00:07:35.935 --> 00:07:37.727
modelling by example

93
00:07:38.239 --> 00:07:42.591
gives us fast feedback about
our understanding of new problem domains

94
00:07:43.103 --> 00:07:49.247
We don't get distracted by
solution domain complexities like

web servers, HTML, and so on

95
00:07:49.503 --> 00:07:53.087
while we're still just trying to understand
the core of the problem

96
00:07:54.367 --> 00:08:00.511
In the long run, we also find that
staying focused on the core domain

97
00:08:00.767 --> 00:08:02.047
helps us build scenarios that are
more stable over time

98
00:08:03.071 --> 00:08:08.703
User interfaces tend to change
a lot more often than

the core business rules of the domain

99
00:08:10.495 --> 00:08:16.639
Building scenarios that make sense
even without a user interface

100
00:08:16.895 --> 00:08:17.407
also helps us to avoid
our tests from becoming too

101
00:08:17.663 --> 00:08:18.943
Imperative

102
00:08:19.967 --> 00:08:26.111
Most teams who write and run their cukes
against the user interface

103
00:08:26.367 --> 00:08:29.183
end it with a lot of incidental detail
in their scenarios about the UI interaction

104
00:08:29.695 --> 00:08:31.231
Solution domain stuff

105
00:08:31.999 --> 00:08:34.559
These scenarios are poor documentation

106
00:08:35.071 --> 00:08:41.215
They're too busy talking about
how the user performs a task

rather than what the user is trying to achieve

107
00:08:42.751 --> 00:08:43.775
For example we might have written
the scenario like this instead

108
00:08:44.031 --> 00:08:46.591
We might have written scenario like this instead

109
00:08:50.175 --> 00:08:52.991
This scenario doesn't illustrate
the behaviour very well

110
00:08:53.759 --> 00:08:59.391
If you didn't know anything about Shouty
and were trying to understand it

111
00:08:59.647 --> 00:09:00.927
through the examples written like this
you'd have a tough time

112
00:09:01.695 --> 00:09:03.999
It makes for lousy documentation

113
00:09:05.535 --> 00:09:11.679
Notice how the language used in this scenario
the URLs, the CSS selectors

114
00:09:11.935 --> 00:09:17.567
even the filling in fields
and clicking of buttons

is from the solution domain,
not the problem domain

115
00:09:18.335 --> 00:09:22.175
It tells you nothing about
your team's ubiquitous language

116
00:09:23.967 --> 00:09:26.527
Finally, this scenario is brittle

117
00:09:26.783 --> 00:09:32.927
If you need to change the details
of the interaction for sending a shout

such as the way you authenticate

118
00:09:33.183 --> 00:09:36.767
you'd need to come back
and change every scenario that involves shouting


119
00:09:37.535 --> 00:09:43.935
By pushing the how down
your scenarios will remain truer for longer

122
00:09:44.703 --> 00:09:50.847
The opposite of the imperative style,
where we express the scenarios

using problem domain language is known as a

123
00:09:51.103 --> 00:09:52.639
Declarative Style

124
00:09:53.151 --> 00:09:59.295
In this style, we try to describe
what the user is trying to achieve

rather than how they do it

125
00:10:00.831 --> 00:10:06.975
Thanks to the declarative style
we've been using through the rest of the series

126
00:10:07.231 --> 00:10:09.535
we were able to easily swap in a
different strategy for automating

127
00:10:09.791 --> 00:10:12.095
our application through the web
without changing our specifications

128
00:10:13.119 --> 00:10:18.495
If there are other interfaces to our application
like a rest API or a mobile app

129
00:10:18.751 --> 00:10:24.895
we can continue to use this pattern,
adding new strategies

130
00:10:25.151 --> 00:10:25.663
that run our scenarios through
these new layers of the stack

131
00:10:26.943 --> 00:10:33.087
Remember, each of these strategies uses
exactly the same scenarios and step definitions

132
00:10:34.367 --> 00:10:40.511
This means the investment you put into
writing your scenarios pays back over and over

133
00:10:40.767 --> 00:10:44.863
as you reuse them to validate
the behaviour of the application

from these different perspectives

134
00:10:45.631 --> 00:10:51.263
This is a major advantage of having
pushed the details of how Sean shouts

down into a helper method

135
00:10:52.031 --> 00:10:57.663
If this detail was still up
in the step definition

136
00:10:57.919 --> 00:10:59.711
or worse, in the scenario itself,
we wouldn't have this flexibility

137
00:11:02.015 --> 00:11:08.159
In general, the structure emerging
in our application and test code

is called a ports and adapters architecture, or

138
00:11:08.415 --> 00:11:10.719
Hexagonal Architecture

139
00:11:11.743 --> 00:11:17.375
You can think of ports and adapters
as a direct analogy

to physical devices with plugs and sockets

140
00:11:18.143 --> 00:11:24.287
For example, the HDMI port on this laptop

141
00:11:24.543 --> 00:11:25.055
lets me plug in any kind of display
that also has a HDMI port

142
00:11:25.823 --> 00:11:30.431
If I need to use a VGA display,
I can use an adapter between the two

143
00:11:31.711 --> 00:11:33.759
In a hexagonal architecture

144
00:11:34.015 --> 00:11:37.343
the inner hexagon contains
your core business logic

145
00:11:38.111 --> 00:11:43.231
This is where the if statements that
deliver the most value to your stakeholders

should live

146
00:11:44.255 --> 00:11:50.399
The inner hexagon knows nothing about
the outside world:

147
00:11:50.655 --> 00:11:52.703
your web servers, your databases, your email
sending service, or your enterprise message bus

148
00:11:53.215 --> 00:11:55.519
It's pure business logic

149
00:11:56.543 --> 00:12:00.895
We expose this core behavior via
one or more ports

150
00:12:01.663 --> 00:12:05.759
A port is really just a protocol,
an API if you like

151
00:12:06.271 --> 00:12:12.415
Any component who understands that protocol
can then plug in and interact

152
00:12:13.695 --> 00:12:15.999
with the core through the port

We call that component an adapter

153
00:12:16.255 --> 00:12:21.119
It's the adapter's job to expose
the core domain logic to the outside world

154
00:12:23.679 --> 00:12:26.239
In Shouty, there's just one port

155
00:12:26.495 --> 00:12:30.847
the API to our domain model
made up of the Person and Network classes

156
00:12:31.615 --> 00:12:34.687
We've plugged in two different adapters
to this little port:

157
00:12:35.199 --> 00:12:41.343
the WebApp, which exposes Shouty's core domain
over the web

158
00:12:41.599 --> 00:12:42.367
for users (or Selenium WebDriver)
to interact with

159
00:12:43.135 --> 00:12:47.999
And the DomainWorld, which lets Cucumber
drive the application directly

161
00:12:48.767 --> 00:12:51.839
Both are clients of the same API

162
00:12:54.143 --> 00:12:59.007
The hexagonal architecture is a terrific fit
for teams who care about testability

163
00:12:59.263 --> 00:13:05.407
In fact, it was invented precisely
to allow for testability

164
00:13:05.663 --> 00:13:07.199
back in the days when thick client GUIs
were impossible to automate

165
00:13:07.711 --> 00:13:13.855
By separating the core domain logic from the GUI

166
00:13:14.111 --> 00:13:16.159
these TDD pioneers were able to plug
their tests into the same port as the GUI

167
00:13:16.415 --> 00:13:19.487
and still test most of
the application's behaviour

168
00:13:20.767 --> 00:13:26.911
If you have the discipline
to keep your code separated like this

you benefit from being able to
run the tests

169
00:13:27.167 --> 00:13:31.263
against the most business-valuable
lines of code in your codebase

170
00:13:31.775 --> 00:13:33.823
the core domain,
in the shortest amount of time

171
00:13:35.103 --> 00:13:41.247
Tests that hit pure business logic
can run lightning fast

meaning it's cheap to get
really thorough feedback

172
00:13:41.503 --> 00:13:44.575
about whether that logic is behaving correctly

173
00:13:45.855 --> 00:13:47.391
Let's get back to the code

174
00:13:47.903 --> 00:13:54.047
To give us a way to test the app manually,
the Shouty developers have kindly added this

config.ru file

175
00:13:54.559 --> 00:13:58.399
which starts the web server pre-configured
with some familiar test data

176
00:13:59.423 --> 00:14:03.519
This means we're able to test
the web app without having

177
00:14:04.031 --> 00:14:05.567
to create accounts for people
a feature we don't have yet!

178
00:14:06.847 --> 00:14:10.175
So we should be able to
open our browser tab as Sean

179
00:14:10.687 --> 00:14:14.783
then another tab as Lucy
send a shout from Sean

181
00:14:15.039 --> 00:14:21.183
refresh Lucy's page and see- 
wait a minute! Where's Sean Shout?

182
00:14:22.463 --> 00:14:23.487
This is odd -

183
00:14:23.743 --> 00:14:28.863
The scenario is passing
but there's nothing appearing on Lucy's page

185
00:14:29.887 --> 00:14:31.167
What's going on?

186
00:14:32.959 --> 00:14:37.311
The answer lies in the implementation
of our Then step

187
00:14:38.591 --> 00:14:44.735
Reading it, we can see that
the step definition is

going directly to the domain model
to discover the messages

188
00:14:44.991 --> 00:14:46.015
that Lucy has heard

189
00:14:47.039 --> 00:14:50.879
This shouldn't have been a surprise to us -
it's always been that way -

191
00:14:51.647 --> 00:14:57.791
but since our When step now hits the UI
we would expect this Then step

192
00:14:58.047 --> 00:14:59.327
to also adopt the same strategy

193
00:15:00.863 --> 00:15:04.447
When you start to mix different
depths of testing as we’re doing here

194
00:15:04.959 --> 00:15:10.591
a good rule of thumb is to keep the depth of your
When and Then steps consistent

195
00:15:11.615 --> 00:15:17.759
It’s often advisable to bypass layers
and reach down deeper into the stack to

196
00:15:18.015 --> 00:15:18.783
set up state in the Given steps

197
00:15:19.295 --> 00:15:22.111
But if we carry out an action via the UI

198
00:15:22.623 --> 00:15:27.231
the outcome check in the Then step
should also be done through the UI

199
00:15:29.535 --> 00:15:33.887
Let’s remedy this situation by
pushing the code in this step definition

200
00:15:34.143 --> 00:15:35.423
into a helper method

201
00:15:35.935 --> 00:15:40.543
Then we’ll be able to have
two different strategies

for checking the messages
that a user has heard

202
00:15:41.567 --> 00:15:46.431
Once we have a failing test for
the web strategy, we can

203
00:15:46.687 --> 00:15:49.247
drive out the behaviour in the UI
to display the user’s messages

204
00:15:50.271 --> 00:15:53.087
We’ll focus on a single scenario
while we do this work

205
00:15:53.855 --> 00:15:57.183
Once we’ve got that one passing
to our satisfaction

206
00:15:57.439 --> 00:16:00.511
we can apply the same change across
the rest of the scenarios

207
00:16:01.791 --> 00:16:06.911
This very basic scenario, where Lucy hears Sean
is a good place to start

208
00:16:10.239 --> 00:16:15.359
Let's extract a method, messages heard by,
from the body of the step definition

209
00:16:15.615 --> 00:16:18.431
and put it in our DomainWorld

210
00:16:29.951 --> 00:16:31.999
That scenario is still passing, good

211
00:16:34.047 --> 00:16:39.167
Now, when we run it through the web

we’re shown we need to add
that method to the WebWorld

212
00:16:40.447 --> 00:16:42.495
How will we fetch the messages heard?

213
00:16:45.823 --> 00:16:50.175
The first thing we’ll need to do is log in

to make sure we’re reading the messages
for the correct user

214
00:16:50.687 --> 00:16:53.247
So we can re-use this
helper method we’ve already built

215
00:16:54.527 --> 00:16:58.111
Now, we’ll need to
scrape the messages off the HTML page

216
00:16:59.135 --> 00:17:02.719
Remember we’re going test-first here,
so we don’t have this markup yet

217
00:17:03.487 --> 00:17:04.511
That's not a problem

218
00:17:04.767 --> 00:17:08.607
We can use the test to help us design
what the markup should look like

219
00:17:10.143 --> 00:17:15.007
Let’s assume that each message will be
an element with a message class on it

220
00:17:16.031 --> 00:17:19.615
We can ask Selenium to give us all
the elements with that class

221
00:17:20.639 --> 00:17:26.783
That gives us a list of
Selenium WebDriver Elements which

222
00:17:27.039 --> 00:17:27.551
we can then transform into a list of their text content

223
00:17:34.975 --> 00:17:36.767
Let's watch this test fail

224
00:17:40.607 --> 00:17:41.631
That looks OK.

225
00:17:42.399 --> 00:17:47.263
Let's play fake it till you make it

just to check this assertion
is doing the right thing

226
00:17:47.775 --> 00:17:51.615
We’ll hard-code the HTML we want
into the template, here

227
00:17:53.151 --> 00:17:56.223
That gives us a chance to
talk with our designer about the markup

228
00:17:56.991 --> 00:18:01.855
We go over for a chat and he loves it,
so we can press on! 

230
00:18:08.255 --> 00:18:11.839
The next step is to
make the HTML template dynamic

231
00:18:12.095 --> 00:18:15.935
and have it display the actual list of
messages heard by the user

232
00:18:16.703 --> 00:18:22.847
We could continue using the
Cucumber scenario to drive this out

233
00:18:23.103 --> 00:18:25.151
but it would be better to zoom in and focus
on some unit tests for the web app now

234
00:18:26.175 --> 00:18:32.319
That way, if this behaviour ever
slips loose in the future

235
00:18:32.575 --> 00:18:33.599
there will be a unit test pointing us
to exactly where we need to go to fix it

236
00:18:34.879 --> 00:18:38.975
Luckily the web app has already been built
with some unit tests around it

237
00:18:40.255 --> 00:18:47.167
These tests load the HttpServlet in isolation
passing in a map of people

239
00:18:47.423 --> 00:18:49.727
that just contains test doubles

240
00:18:50.239 --> 00:18:56.383
We could have used real instances
from our domain model

but as we explained in the last episode,
using test doubles helps us to see

241
00:18:56.639 --> 00:19:02.527
the protocol on the port between the web app
adapter and our core domaine

242
00:19:03.551 --> 00:19:07.647
We’ve abstracted some of the nitty-gritty
of calling the Servlet into a base class

243
00:19:08.159 --> 00:19:09.951
which you can examine for yourself

244
00:19:15.583 --> 00:19:21.727
We need a new test for the GET request,
which simulates the situation where

Lucy has heard a couple of messages

245
00:19:22.495 --> 00:19:24.031
and views her homepage

246
00:19:43.999 --> 00:19:48.095
We’d expect to be able to find the message text
in each of the message elements

247
00:19:54.751 --> 00:19:59.871
When we run this, it fails because
we’re just hard-coding the message at the moment

248
00:20:00.383 --> 00:20:02.175
Let’s TDD our solution

249
00:20:04.223 --> 00:20:10.367
If you’d like to try this yourself,
just pause the video here and

see if you can figure out what to do next,
before we show you

250
00:20:12.927 --> 00:20:18.559
Starting in the template, we can look for
a local variable, let’s call it messages heard

251
00:20:18.815 --> 00:20:20.351
and iterate over it

252
00:20:20.607 --> 00:20:23.423
For each message, we’ll write a message element

253
00:20:26.495 --> 00:20:29.823
Now we need to set that messages heard
variable for the template

254
00:20:30.079 --> 00:20:34.687
We do that from within the doGet request handler
method in the web servlet

255
00:20:35.967 --> 00:20:39.551
We need to put the messages heard into this context map

256
00:20:40.319 --> 00:20:46.463
Helpfully we have this getUser method that
fetches the right user from the people map

based on the query string parameter used in the request

257
00:20:46.719 --> 00:20:50.047
Let’s give that a try

258
00:20:51.583 --> 00:20:55.679
Great! Suddenly everything is green

260
00:20:56.959 --> 00:21:03.103
Try a quick manual test for yourself,
creating a couple of tabs as Sean and Lucy

261
00:21:03.359 --> 00:21:04.895
and satisfy yourself that it’s working now

262
00:21:05.919 --> 00:21:10.783
Now that we’ve proved our
messages_heard_by method

works for both domain and web strategies

263
00:21:11.039 --> 00:21:13.855
let’s use that method in
all the step definitions

264
00:21:14.111 --> 00:21:19.744
so that every scenario
that checks for messages heard

will do that check in a consistent way

265
00:21:20.768 --> 00:21:26.912
This is just a matter of finding each call
to ask a Person domain object for messagesHeard

266
00:21:27.168 --> 00:21:28.704
and converting it to use
our new helper method instead

267
00:21:32.032 --> 00:21:34.592
Let's run all the scenarios in the shout feature

268
00:21:48.928 --> 00:21:51.744
Great. It looks like we're done

270
00:21:52.512 --> 00:21:58.656
If we look at the other feature
Premium accounts

271
00:21:58.912 --> 00:21:59.680
we can see that there’s
a similar problem to the one we’ve just resolved

272
00:22:00.192 --> 00:22:04.800
This last step
'Then Sean should have n credits'

274
00:22:05.056 --> 00:22:11.200
goes direct to the domain model
to check Sean’s credits

rather than having that
extra layer of indirection

275
00:22:11.456 --> 00:22:14.784
that would allow us to use a domain
or a web strategy for the check

276
00:22:15.808 --> 00:22:21.952
It will be useful practice for you
to go through and apply

exactly what we just did to this step definition

277
00:22:22.976 --> 00:22:25.280
We'll leave that as an exercise for you

278
00:22:27.328 --> 00:22:29.632
Let's fast-forward to the point
where this is done

279
00:22:31.168 --> 00:22:37.312
Now, we can run all of our scenarios
at both levels

280
00:22:37.568 --> 00:22:38.336
against the domain, and against the web UI

281
00:22:39.616 --> 00:22:41.152
This is awesome!

282
00:22:42.432 --> 00:22:43.456
Isn't it?

283
00:22:43.968 --> 00:22:47.552
We've talked a lot about the benefits of
automated tests in this series

284
00:22:47.808 --> 00:22:51.904
but let’s consider the flip side for a moment,
and look at the costs

285
00:22:53.440 --> 00:22:57.280
Every automated test in your system
comes at a cost

286
00:22:58.048 --> 00:23:00.608
You have the cost of 
writing it in the first place

287
00:23:00.864 --> 00:23:03.936
the cost of waiting for it to run each time

288
00:23:04.448 --> 00:23:08.800
the cost of changing it when the
desired behaviour of the application changes

289
00:23:09.056 --> 00:23:13.408
and the cost of debugging it
when it fails for no good reason

290
00:23:14.944 --> 00:23:21.088
When the majority of your tests hit
the application through the user interface

you get a great benefit

291
00:23:21.344 --> 00:23:24.928
from knowing that each scenario
is using the system exactly as a user would

292
00:23:25.696 --> 00:23:29.536
Yet the downside is that
these tests are much slower to run

293
00:23:29.792 --> 00:23:31.840
and are often also much less reliable

294
00:23:33.376 --> 00:23:39.008
A well-known metaphor to help you
think about this problem is the

Agile Testing Pyramid

295
00:23:40.032 --> 00:23:44.128
At the base of the pyramid,
you have a large number of unit tests

296
00:23:44.896 --> 00:23:51.040
shallow tests that directly hit isolated,
individual classes and modules in your solution

297
00:23:52.832 --> 00:23:58.976
The pyramid gets narrower as you go up
indicating that as the depth of tests increases

298
00:23:59.232 --> 00:24:01.024
the less of them you should have

299
00:24:01.536 --> 00:24:07.168
At the very top of the pyramid,
where the tests go right through

the whole application stack

300
00:24:07.680 --> 00:24:09.472
you want as few as possible

301
00:24:10.496 --> 00:24:13.824
Just enough to give you confidence
the thing is working

302
00:24:15.616 --> 00:24:18.176
When you drive most of
your behaviour through the GUI

303
00:24:18.688 --> 00:24:23.296
you end up with the opposite -
more of a testing ice cream cone

305
00:24:24.064 --> 00:24:29.440
Now I love an ice cream on a hot summer’s day
but when your test suite looks like this

307
00:24:29.696 --> 00:24:35.840
you’re waiting hours for test results,
and there are generally at least

308
00:24:36.096 --> 00:24:37.120
one or two random failures in every build

309
00:24:40.960 --> 00:24:44.288
Although we have the choice to
run every Shouty scenario through the GUI

310
00:24:44.544 --> 00:24:48.640
the Agile Testing Pyramid tells us
that would be a bad idea

311
00:24:49.664 --> 00:24:55.808
We need to select a few representative or
key examples to run through the web UI

312
00:24:56.064 --> 00:24:58.368
and run the rest through the domain

313
00:24:59.392 --> 00:25:01.440
How do we choose those key examples?

314
00:25:02.976 --> 00:25:05.792
Let’s try and think about
what could possibly go wrong

315
00:25:06.560 --> 00:25:11.424
We want to identify the minimum number of
scenarios that would give us confidence

the system is working

316
00:25:11.936 --> 00:25:17.312
Remember: both our core domain
and our web server

317
00:25:17.568 --> 00:25:20.384
so we just need a few checks
for basic correctness

318
00:25:20.896 --> 00:25:26.272
Would it be enough to just test this scenario
the one where the listener is within range?

320
00:25:27.296 --> 00:25:30.112
If we did that, what could possibly go wrong?

321
00:25:31.648 --> 00:25:37.792
With our tester’s hat on
we can imagine a bug where

the web server’s template
didn’t render multiple messages

322
00:25:38.560 --> 00:25:40.608
This scenario only works for one

323
00:25:41.632 --> 00:25:45.472
So we could add this scenario,
the one where there are two shouts

324
00:25:46.240 --> 00:25:52.128
Yet we’d easily catch that bug in manual testing

and could then pin it down with
a unit test on the web server

325
00:25:53.408 --> 00:25:56.736
So we don’t need to run this as a
full-stack test every time

326
00:25:57.504 --> 00:25:58.528
In fact

327
00:25:58.784 --> 00:26:01.344
to do so would be wasteful

328
00:26:03.136 --> 00:26:06.720
How about the scenario about
the listener being out of range?

329
00:26:06.976 --> 00:26:10.304
If we skip that in our web-depth Cucumber run

330
00:26:11.072 --> 00:26:13.888
would we leave ourselves vulnerable
to a dangerous bug?

331
00:26:15.424 --> 00:26:21.568
Well, it’s true that if people heard messages
that were not meant for them

it could make us look pretty bad

332
00:26:22.592 --> 00:26:24.640
How likely is this to happen?

333
00:26:25.408 --> 00:26:27.456
That logic is all in the core domain

334
00:26:27.968 --> 00:26:31.552
the web server just renders
the messages returned by the core

335
00:26:32.320 --> 00:26:35.904
So there’s almost a zero risk of
this bug ever leaking out

336
00:26:36.672 --> 00:26:40.256
Again, a full-stack test for this
behaviour would be wasteful

337
00:26:42.816 --> 00:26:48.960
Although the same goes for
the logic about a long message

we know that there’s potential for
there to be bugs in the interaction

338
00:26:49.216 --> 00:26:51.520
of the UI for longer messages

339
00:26:51.776 --> 00:26:53.824
so it makes sense for us to run this one

340
00:26:54.592 --> 00:26:56.896
Let’s mark it up as @high-risk

341
00:26:57.408 --> 00:27:01.504
and mark the one where the listener is
within range as @high-impact

342
00:27:02.272 --> 00:27:04.064
We’ll explain those terms in a moment

343
00:27:05.344 --> 00:27:12.256
Now, we can run the tests at the top of our pyramid
using those tags and the web test depth setting

345
00:27:13.280 --> 00:27:17.888
Now we have three different levels of tests in
our pyramid that we want to run to check our code

The unit tests

346
00:27:18.144 --> 00:27:25.312
the core domain acceptance tests,
and the key examples running as full-stack tests

348
00:27:27.360 --> 00:27:31.968
Let's set up our maven configuration to run all of these tests first with a single command

349
00:27:33.248 --> 00:27:38.880
Maven already runs our unit and domain-level
acceptance tests automatically

using the surefire plugin

350
00:27:39.392 --> 00:27:45.536
We can add extra configuration to our pom.xml
to tell surefire to run a second execution

351
00:27:45.792 --> 00:27:47.072
of the tests with different configuration

352
00:27:48.352 --> 00:27:50.912
Just add a surefire plugin node like this

353
00:27:51.680 --> 00:27:56.032
then an execution node with an id of
web-acceptance-tests

354
00:27:57.312 --> 00:28:01.408
Now we attach it to the test goal -
the one maven runs by default

356
00:28:02.432 --> 00:28:05.760
We configure the execution by excludin
all the unit tests - no need to run those again -

357
00:28:06.016 --> 00:28:07.552
No need to run those again

358
00:28:07.808 --> 00:28:11.136
and setting the argline to use
the testDepth of web

359
00:28:11.392 --> 00:28:16.256
and with the cucumber options set to run
only those scenarios tagged as either

high impact or high risk

360
00:28:18.816 --> 00:28:24.448
There. Now when we run `mvn test`
from the command-line

361
00:28:24.704 --> 00:28:26.496
it runs all three layers of our testing pyramid
starting from the bottom up

362
00:28:31.104 --> 00:28:32.384
One last thing

363
00:28:32.896 --> 00:28:37.504
When we automate web pages, we need to
refer to user interface elements and actions

364
00:28:37.760 --> 00:28:40.320
buttons, links, text, clicks, etc

365
00:28:41.088 --> 00:28:46.464
This is solution domain jargon
we’ve managed to keep out of our scenarios

and that’s good!

366
00:28:47.232 --> 00:28:48.768
But it has to go somewhere

367
00:28:50.304 --> 00:28:56.448
On larger projects, it becomes useful
to create abstractions called

368
00:28:56.704 --> 00:28:58.240
page objects to represent
the things being filled in

clicked, and examined for their content

369
00:28:58.752 --> 00:29:04.896
For example, we might have a homepage object
with a method called shout

370
00:29:05.152 --> 00:29:07.456
that encapsulated the calls to interact
with the elements on the web page

371
00:29:07.968 --> 00:29:12.576
This allows you to easily reuse code
and keep it easy to read

373
00:29:13.088 --> 00:29:15.648
You can easily build page objects on your own

374
00:29:15.904 --> 00:29:22.048
but if you’re feeling lazy
Selenium ships with a PageFactory class

375
00:29:22.304 --> 00:29:23.072
that reduces the amount of boilerplate code
you need to write

376
00:29:24.608 --> 00:29:29.984
The page objects pattern is a great fit
for keeping your web automation code tidy

377
00:29:30.240 --> 00:29:31.008
but we strongly recommend that you try to

378
00:29:31.264 --> 00:29:37.408
push as many of your tests
down the pyramid first

379
00:29:38.432 --> 00:29:43.552
You can use page objects together with
the hexagonal architecture pattern

380
00:29:43.808 --> 00:29:46.368
we’ve shown you in this episode
so that when you do need to hit the web UI

381
00:29:46.624 --> 00:29:49.440
you do that through neat and tidy code

382
00:29:50.208 --> 00:29:53.024
That's it kids, school's out!

383
00:29:53.280 --> 00:29:55.840
Time to step out into the real world

384
00:29:57.120 --> 00:30:02.240
This has been an intense episode
and we’ve thrown a lot of new concepts at you -

385
00:30:02.752 --> 00:30:08.128
the hexagonal architecture, the strategy
pattern and the agile testing pyramid

386
00:30:09.408 --> 00:30:15.552
Please take time to watch this video over
a few times until you understand these concepts

387
00:30:15.808 --> 00:30:18.112
and study the exercises and reflection questions

388
00:30:19.136 --> 00:30:25.024
We want you to remember that acceptance tests
don’t have to be full-stack tests

389
00:30:25.792 --> 00:30:28.608
In fact, it’s often a mistake if they are

391
00:30:29.888 --> 00:30:35.520
Don’t fall into the trap of building yet another
testing ice cream cone for your project

392
00:30:36.032 --> 00:30:42.176
Have developers and testers work side-by-side

to maximize the value you get
from your testing investment

393
00:30:42.432 --> 00:30:47.552
by pushing as many tests
as you can down to the lowest level

394
00:30:48.576 --> 00:30:54.720
The responsibility for the health and wellbeing
of the world’s Cucumber suites rests with you now

395
00:30:55.744 --> 00:30:58.304
Use your knowledge wisely!

396
00:31:00.864 --> 00:31:05.472
If you remember one thing from
the video series, remember this:

397
00:31:06.496 --> 00:31:09.824
The software you write is just a model -

399
00:31:10.848 --> 00:31:14.688
a model of your team's understanding
of the problem domain

400
00:31:15.712 --> 00:31:20.320
The better that understanding is
the better the software will be

402
00:31:21.600 --> 00:31:25.696
Put your effort into understanding
the problem together,

403
00:31:26.208 --> 00:31:29.536
and the software will take care of itself

405
00:31:31.840 --> 00:31:34.144
Goodbye from all of us on Cucumber School

406
00:31:34.656 --> 00:31:36.192
and have fun out there!

471
00:31:36.500 --> 00:30:38.500
Captions created by Jayson Smith for Cucumber Ltd.
